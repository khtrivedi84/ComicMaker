{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "830c0bb0-a2b7-471c-aaf2-3d9d8e32af2f",
   "metadata": {},
   "source": [
    "# Dependencies and HuggingFace Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b18c5bf4-3865-464c-b40f-337a7811346e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting openai\n",
      "  Downloading openai-0.27.7-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.28.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.20->openai) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.20->openai) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.20->openai) (2019.11.28)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (22.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
      "Installing collected packages: openai\n",
      "Successfully installed openai-0.27.7\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Installing dependencies\n",
    "\n",
    "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/examples/dreambooth/train_dreambooth.py\n",
    "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n",
    "%pip install -qq git+https://github.com/ShivamShrirao/diffusers\n",
    "%pip install -q -U --pre triton\n",
    "%pip install -q accelerate transformers ftfy bitsandbytes==0.35.0 gradio natsort safetensors xformers\n",
    "!pip install openai\n",
    "import os\n",
    "import openai\n",
    "openai.api_key = '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'\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import textwrap\n",
    "\n",
    "# Login to HuggingFace ðŸ¤—\n",
    "!mkdir -p ~/.huggingface\n",
    "HUGGINGFACE_TOKEN = \"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\"     # https://huggingface.co/settings/tokens\n",
    "!echo -n \"{HUGGINGFACE_TOKEN}\" > ~/.huggingface/token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c7f8fe-f354-4f80-b806-78a1f7e786b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creating Directory for saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5665e318-ea37-41bc-9c12-afadd2e7ab4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Weights will be saved at stable_diffusion_weights/kht\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR = \"stable_diffusion_weights/kht\"\n",
    "\n",
    "print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")\n",
    "\n",
    "!mkdir -p $OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be2dec6-5995-4458-bbd7-fa43a5f8a074",
   "metadata": {},
   "source": [
    "# Model Training (Hyperparameter Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e969bdf6-e266-4315-b018-cf42b0d9bf4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 train_dreambooth.py \\\n",
    "  --pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\" \\\n",
    "  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
    "  --output_dir=$OUTPUT_DIR \\\n",
    "  --instance_prompt=\"photo of kht person\" \\\n",
    "  --class_prompt=\"photo of a person\" \\\n",
    "  --instance_data_dir=\"instance_images\" \\\n",
    "  --class_data_dir=\"class_images\" \\\n",
    "  --save_sample_prompt=\"photo of kht person\" \\\n",
    "  --revision=\"fp16\" \\\n",
    "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
    "  --seed=1337 \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=2 \\\n",
    "  --train_text_encoder \\\n",
    "  --mixed_precision=\"fp16\" \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --learning_rate=1e-6 \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --num_class_images=250 \\\n",
    "  --sample_batch_size=4 \\\n",
    "  --max_train_steps=1000 \\\n",
    "  --save_interval=5000 \\\n",
    "  --save_sample_prompt=\"photo of kht guy\"\n",
    "\n",
    "# Reduce the `--save_interval` to lower than `--max_train_steps` to save weights from intermediate steps.\n",
    "# `--save_sample_prompt` can be same as `--instance_prompt` to generate intermediate samples (saved along with weights in samples directory)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fb517e-b167-4f24-94c7-cca05792b1a1",
   "metadata": {},
   "source": [
    "## Selecting Pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f30d1fc9-bcd6-49cf-bb7f-d8b8c742ce17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] WEIGHTS_DIR=10001\n"
     ]
    }
   ],
   "source": [
    "# Specify the weights directory to use (leave blank for latest)\n",
    "WEIGHTS_DIR = \"10001\" #@param {type:\"string\"}\n",
    "if WEIGHTS_DIR == \"\":\n",
    "    from natsort import natsorted\n",
    "    from glob import glob\n",
    "    import os\n",
    "    WEIGHTS_DIR = natsorted(glob(OUTPUT_DIR + os.sep + \"*\"))[-1]\n",
    "print(f\"[*] WEIGHTS_DIR={WEIGHTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329b9e72-2ad4-48ae-8d22-3d5d0f0f1af7",
   "metadata": {},
   "source": [
    "## Saving the .ckpt file to use in future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a9b08d-d4c6-409b-bcd8-04195a3c97a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code to convert the model into .ckpt format to use in WwebUI like Automatic1111\n",
    "ckpt_path = WEIGHTS_DIR + \"/model.ckpt\"\n",
    "!ls\n",
    "half_arg = \"\"\n",
    "\n",
    "# Whether to convert to fp16, takes half the space (2GB).\n",
    "fp16 = True #@param {type: \"boolean\"}\n",
    "if fp16:\n",
    "    half_arg = \"--half\"\n",
    "!python convert_diffusers_to_original_stable_diffusion.py --model_path $WEIGHTS_DIR  --checkpoint_path $ckpt_path $half_arg\n",
    "print(f\"[*] Converted ckpt saved at {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf2076f-5440-46b2-ad2a-fdf5cdb9e1b3",
   "metadata": {},
   "source": [
    "# Model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b42babcf-6c89-4727-b038-aceecb6c4db0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import autocast\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "from IPython.display import display\n",
    "\n",
    "model_path = WEIGHTS_DIR             # If you want to use previously trained model saved in gdrive, replace this with the full path of model in gdrive\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_path, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "# pipe.enable_xformers_memory_efficient_attention()\n",
    "g_cuda = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96110b7b-1684-4a15-bf2b-4eb86845d48b",
   "metadata": {},
   "source": [
    "# Setting seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bcc74b7-a82c-4feb-a140-468da43b008f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc82c0b75d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can set random seed here for reproducibility.\n",
    "\n",
    "g_cuda = torch.Generator(device='cuda')\n",
    "seed = 4254147815 #@param {type:\"number\"}\n",
    "g_cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eed89c-aca0-499e-8f00-38cc8db53ca3",
   "metadata": {},
   "source": [
    "# Comic generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1481bad-7c59-4599-8785-0232190504eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your clothing style (Iron man armor, Batman Suit, Black Suit:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Aquaman suit and holding Aquaman trident\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a style for the images (Watercolor painting, By van gogh or leave blank for realistic): \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " oil painting\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c5879a11e148648edee10e380a3816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "539f3941544449adace5cc9a964aaa6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cbd4d5579f24cb2a1b2d42e0e052fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d8900680a14f9292c6c399374a6ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf7d5b121674438bb590d3cb5d6ea5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_170/1291792333.py:64: DeprecationWarning: getsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use getbbox or getlength instead.\n",
      "  wrapper = textwrap.TextWrapper(width=int(max_line_width / font.getsize(\" \")[0]))\n",
      "/tmp/ipykernel_170/1291792333.py:68: DeprecationWarning: getsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use getbbox or getlength instead.\n",
      "  line_height = font.getsize(\" \")[1]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "print(\"Enter your clothing style (Iron man armor, Batman Suit, Black Suit:\")\n",
    "clothing_style = input()\n",
    "print(\"Enter a style for the images (Watercolor painting, By van gogh or leave blank for realistic): \")\n",
    "image_type = input()\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a short story writer and a prompt generator. You have to write a story of a character called kht person. There is only one character in the story. The person is a male. I'll give you a sample story to make you understand. A vigilant gets some information about a group of people trying to destroy earth. They are in a building on 27th floor. The vigilant, then, goes to the building. Then he sits in the lift. Once he reaches floor 27, he throws a grenade in a room where all the data related to the group was destroyed and then he started running across the hall and then goes outside the building. Then he sees the building burning and then he goes back to his safe-house where he stand on rooftop of the building and thinks that the world is safe now. So make a story in this format but using only 5 dialogues. before each dialogue write Dialogue: and before each prompt write Prompt: The prompt should be related to the dialogue. After each dialogue, you have to write a prompt for a text-to-image generation tool. First write the dialogue and then in each prompt, you should first mention 'A close-up shot of kht person' then you have to mention what kht person is wearing a {}, and lastly mention what is the background. Remember that for every prompt, whatever kht person is wearing should be same.\".format(clothing_style)}\n",
    "  ]\n",
    ")\n",
    "\n",
    "text = completion.choices[0].message.content\n",
    "\n",
    "# Splitting the text into individual lines\n",
    "lines = text.strip().split('\\n')\n",
    "        \n",
    "dialogue_pattern = r'Dialogue: \"(.*?)\"'\n",
    "prompt_pattern = r'Prompt: (.*?)\\.'\n",
    "\n",
    "dialogues = re.findall(dialogue_pattern, text)\n",
    "prompts = re.findall(prompt_pattern, text)\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    prompt = \"{}, {}\".format(image_type, prompts[i])\n",
    "    negative_prompt = \"\"\n",
    "    num_samples = 1 \n",
    "    guidance_scale = 7.8\n",
    "    num_inference_steps = 100 \n",
    "    height = 512 \n",
    "    width = 512 \n",
    "\n",
    "    with autocast(\"cuda\"), torch.inference_mode():\n",
    "        images = pipe(\n",
    "            prompt,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            negative_prompt=negative_prompt,\n",
    "            num_images_per_prompt=num_samples,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            generator=g_cuda\n",
    "        ).images\n",
    "\n",
    "    for img in images:\n",
    "        img.save('image{}.jpg'.format(i))\n",
    "for i in range(5):\n",
    "    image = Image.open('image{}.jpg'.format(i))\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font = ImageFont.truetype('arial.ttf', size=20)\n",
    "    text = dialogues[i]\n",
    "\n",
    "    # Calculate available width and height for the text rectangle\n",
    "    available_width = image.width - 20  # Subtracting margin on both sides\n",
    "    available_height = image.height - 20  # Subtracting margin on both sides\n",
    "\n",
    "    # Calculate the maximum width and height for each line of wrapped text\n",
    "    max_line_width = available_width\n",
    "    max_line_height = available_height\n",
    "\n",
    "    # Wrap the text to fit within the available width\n",
    "    wrapper = textwrap.TextWrapper(width=int(max_line_width / font.getsize(\" \")[0]))\n",
    "    wrapped_text = wrapper.wrap(text)\n",
    "\n",
    "    # Calculate the height required to display the wrapped text\n",
    "    line_height = font.getsize(\" \")[1]\n",
    "    text_height = len(wrapped_text) * line_height\n",
    "\n",
    "    # Specify the position of the text rectangle\n",
    "    text_x = 5  # Adjust the X position according to your requirement\n",
    "    text_y = 10  # Adjust the Y position according to your requirement\n",
    "\n",
    "    # Draw the rectangle with a white background\n",
    "    draw.rectangle([(text_x, text_y), (text_x + 20 + available_width, text_y + text_height + line_height)], fill='#C4C6D4')\n",
    "\n",
    "    # Draw the wrapped text with black font color\n",
    "    text_position = (text_x + 10, text_y + 10)  # Adjust the padding according to your requirement\n",
    "    for line in wrapped_text:\n",
    "        draw.text(text_position, line, (0, 0, 0), font=font)  # Use black color for the text\n",
    "        text_position = (text_position[0], text_position[1] + line_height)\n",
    "\n",
    "    # Save image\n",
    "    image.save('test{}.jpg'.format(i))\n",
    "\n",
    "# I have 5 images of different sizes and I want to merge them into one image in a style of a comic book grid.\n",
    "# I want to merge them in a way that the images are not stretched or distorted. I want to keep the aspect ratio of the images.\n",
    "\n",
    "# In the grid, first row should have 2 images, second row should have 1 image and third row should have 2 images.\n",
    "\n",
    "# open the images\n",
    "image1 = Image.open('test0.jpg')\n",
    "image2 = Image.open('test1.jpg')\n",
    "image3 = Image.open('test2.jpg')\n",
    "image4 = Image.open('test3.jpg')\n",
    "image5 = Image.open('test4.jpg')\n",
    "# image6 = Image.open('test6.jpg')\n",
    "\n",
    "# resize images\n",
    "image1 = image1.resize((400, 400))\n",
    "image2 = image2.resize((400, 400))\n",
    "image3 = image3.resize((400, 400))\n",
    "image4 = image4.resize((400, 400))\n",
    "image5 = image5.resize((400, 400))\n",
    "# image6 = image6.resize((400, 400))\n",
    "\n",
    "# create a new empty image, RGB mode, and size 400 by 400\n",
    "# Open the background image\n",
    "background_image = Image.open('background_image.png')\n",
    "background_image = background_image.resize((800, 1200))\n",
    "\n",
    "# Create a new image with the same size as the background image\n",
    "new_image = Image.new('RGB', (800, 1200))\n",
    "\n",
    "# Paste the background image\n",
    "new_image.paste(background_image, (0, 0))\n",
    "\n",
    "# Make the background image semi-transparent\n",
    "new_image = new_image.point(lambda x: x * 0.5)\n",
    "\n",
    "# paste image giving dimensions\n",
    "new_image.paste(image1, (0, 0))\n",
    "new_image.paste(image2, (400, 0))\n",
    "new_image.paste(image3, (200, 400))\n",
    "new_image.paste(image4, (0, 800))\n",
    "new_image.paste(image5, (400, 800))\n",
    "# new_image.paste(image6, (400, 1200))\n",
    "\n",
    "# save the image\n",
    "new_image.save('merged_imag3.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69de0f03-ff3d-4d19-ac5c-8b69f00e76dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
